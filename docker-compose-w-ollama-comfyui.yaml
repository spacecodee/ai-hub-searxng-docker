services:
  caddy:
    container_name: caddy
    image: docker.io/library/caddy:2-alpine
    network_mode: host
    restart: unless-stopped
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy-data:/data:rw
      - caddy-config:/config:rw
    environment:
      - SEARXNG_HOSTNAME=${SEARXNG_HOSTNAME:-http://localhost}
      - SEARXNG_TLS=${LETSENCRYPT_EMAIL:-internal}
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  redis:
    container_name: redis
    image: docker.io/valkey/valkey:8-alpine
    command: valkey-server --save 30 1 --loglevel warning
    restart: unless-stopped
    networks:
      - searxng
    volumes:
      - valkey-data2:/data
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
      - DAC_OVERRIDE
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  searxng:
    container_name: searxng
    image: docker.io/searxng/searxng:latest
    restart: unless-stopped
    networks:
      - searxng
    ports:
      - "8080:8080"
    volumes:
      - ./searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
      - UWSGI_WORKERS=${SEARXNG_UWSGI_WORKERS:-4}
      - UWSGI_THREADS=${SEARXNG_UWSGI_THREADS:-4}
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    logging:
      driver: "json-file"
      options:
        max-size: "4m"
        max-file: "1"

  ollama:
    volumes:
      - ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    tty: true
    restart: unless-stopped
    image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}
    networks:
      - searxng

  openwebui:
    container_name: openwebui
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    networks:
      - searxng
    ports:
      - "3000:8080"
    environment:
      - CORS_ALLOW_ORIGINS=*
      - COMFYUI_BASE_URL=http://comfyui:8188
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - searxng
      - ollama
      - comfyui
    volumes:
      - openwebui-data:/app/backend/data
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  comfyui:
    container_name: comfyui
    build:
      context: .
      dockerfile: comfyui/Dockerfile
    ports:
      - "8188:8188"  # Default ComfyUI port
    volumes:
      - ./models:/app/models  # Mount models directory
      - ./output:/app/output  # Mount output directory
      - ./input:/app/input    # Mount input directory
      - ./custom_nodes:/app/custom_nodes  # Mount custom nodes
    networks:
      - searxng
    environment:
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    restart: unless-stopped

networks:
  searxng:

volumes:
  caddy-data:
  caddy-config:
  valkey-data2:
  openwebui-data: { }
  ollama: { }